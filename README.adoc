= Change Data Capture with Debezium (on OpenShift 4.3) =

This demo shows how you can install Debezium on an OpenShift 4.3 (and above) cluster.

There is a small app at the heart of the demo that is writing to a MySQL database (it's added to this repo as a submodule).  

image:images/original-app.png[]

We start with that app and slowly add the following:

1. Use operators to install AMQ Streams on the cluster (pre-requisite for Debezium)
2. Setup KafkaConnectS2I
3. Build and install the Debezium Connector for MySQL
4. Demonstrate change events coming through Debezium 
5. Use fuse to transform events coming out of the Database for feeding into Elastisearch
6. Publish connector metrics to central OpenShift Monitoring
7. Install a custom grafana dashboard to visualize metrics

image:images/overall-arch.png[]

[NOTE]
.This repo has sub-modules
====
To clone this repo as required, you will need to issue this command

----
git clone --recurse-submodules https://github.com/hatmarch/cdc-and-debezium-demo.git
----

Where _https://github.com/hatmarch/cdc-and-debezium-demo.git_ is assumed to be the repo name

====

[NOTE]
====
Commands assume that you have run this command from the root of the git repo.

----
source scripts/shell-setup.sh
----
====

== Initial Project and App Setup ==

First log into an OpenShift 4.3 cluster using an account with cluster-admin privileges.

First thing that needs to be done is to deploy the original application including a MySQL database that is ready to be connected to Debezium.

----
$DEMO_HOME/scripts/00-setup-app.sh
----

As part of this process, the current user will be granted `anyuid` scc privileges as this is required for the debezium provided databasefootnote:database[MySQL databases need to be configured to write their transactions to the rowlevel `binlog` (among other things).  The MySQL database that is used in this demo is from a debezium image and is already setup in this way.  If you use a different MySQL image you will need to ensure the MySQL database is configured correctly as outlined link:https://debezium.io/documentation/reference/1.0/connectors/mysql.html#setting-up-mysql[here]]

The script will also create the database and check that the schema has been created correctly.

If all went well, the last lines of output should look like this:footnote:[If output does not match then you can look into link:/scripts/00-setup-app.sh[the shell script] for where things went wrong and run the commands manually]

----
MYSQL Pod is: mysql-1-r5kb4
mysql: [Warning] Using a password on the command line interface can be insecure.
mysql: [Warning] Using a password on the command line interface can be insecure.
count(*)
0
--> Found Docker image 88d914b (7 months old) from quay.io for "quay.io/quarkus/ubi-quarkus-native-s2i:19.0.2"

    Quarkus.io S2I (GraalVM Native) 
    ------------------------------- 
    Quarkus.io S2I image for building Kubernetes Native Java GraalVM applications and running its Native Executables

    Tags: builder, java, quarkus, native

    * An image stream tag will be created as "ubi-quarkus-native-s2i:19.0.2" that will track the source image
    * A source build using binary input will be created
      * The resulting image will be pushed to image stream tag "quarkus-transaction-crud:latest"
      * A binary build was created, use 'oc start-build --from-dir' to trigger a new build
    * This image will be deployed in deployment config "quarkus-transaction-crud"
    * Port 8080/tcp will be load balanced by service "quarkus-transaction-crud"
      * Other containers can access this service through the hostname "quarkus-transaction-crud"

--> Creating resources ...
    imagestream.image.openshift.io "ubi-quarkus-native-s2i" created
    imagestream.image.openshift.io "quarkus-transaction-crud" created
    buildconfig.build.openshift.io "quarkus-transaction-crud" created
    deploymentconfig.apps.openshift.io "quarkus-transaction-crud" created
    service "quarkus-transaction-crud" created
--> Success
    Use 'oc start-build quarkus-transaction-crud' to start a build.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/quarkus-transaction-crud' 
    Run 'oc status' to view your app.
buildconfig.build.openshift.io/quarkus-transaction-crud patched
Uploading directory "/workspaces/cdc-and-debezium-demo/demo-crud-app" as binary input for the build ...
...........................
route.route.openshift.io/quarkus-transaction-crud exposed
Transaction generating application can be found at http://quarkus-transaction-crud-debezium-cdc.apps.cluster-mel-dbz-2189.mel-dbz-2189.example.opentlc.com/
----

Click on the link provided and you should see the app from the introduction

image:images/original-app.png[]

== Installing AMQ Streams ==

AMQ Streams is easily setup using an Red Hat's AMQ Operator.  This can be installed manually or can be done using the provided script.  Debezium will leverage Kafka's KafkaConnect functionality to allow changes sourced from the target database to be turned into consumeable events.

----
$DEMO_HOME/scripts/01-setup-kafka.sh
----

The script will do the following:

* Install amqstreams.v1.3.0 operator
* Create a kafka cluster in the debezium-cdc project
* install a secret for accessing quay.io (this is where the kafkaconnects2i image lives)
* Create the KafkaConnectS2I instance (for building the debezium connector)

== Building Debezium Connector ==

Now that the KafkaConnectS2I builder is installed, we can start a binary build with all our connector information.

This repo already has a local version of all the plugins the debezium connector needs at `$DEMO_HOME\kube\kafka\connect-plugins` but if you want to see where they were downloaded from (or want to update the contents of this directory) you can do the following:

----
rm -rf $DEMO_HOME/kube/kafka/connect-plugins
$DEMO_HOME/scripts/download-plugins.sh
----

Once we have our debezium plugins downloaded (which includes the MySQL plugin that we'll use to connect to the demo app's database we set up previously) we can create the connector

----
$DEMO_HOME/scripts/02-setup-debezium-connector.sh
----

This script will do the following:

* build the debezium kafka connector from what's in the connect-plugins directory
* Create a route to the connect api
* Check that the debezium kafka connector can be reached
* Register and configure a new connector at the end point called `debezium-connector-mysql`

Here are some of the aspects that are configured:

image:images/connector-config.png[]

If the connector is setup correctly it will create a number of different topics based on the database and the events it's been configured to look for.  For more information see the link:www.debezium.io[official web site]

[TIP]
====
You can a list of all currently registered topics (including those registered by the connector) by running this command
----
oc exec -c kafka my-cluster-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 -n debezium-monitoring --list
----
====

=== Common issues ===

==== Image Registry issues ====

If you see this error:

----
The ImageStreamTag "my-connect-cluster-connect-source:1.3.0" is invalid: from: Error resolving ImageStreamTag my-connect-cluster-connect-source:1.3.0 in namespace debezium-cdc: unable to find latest tagged image
----

It's probably an issue with your registry.io credentials.  Open the ImageStreams tab of the project and look at the `my-connect-cluster-connect-source` image stream.  If you see a warning at the top that when expanded looks like this:

image:images/image-stream-issue.png[]

Then you likely have an issue with the secret that was provided to log into registry.io.  Check your login details and update the secret `connects2i` and re-run the script

== Testing Connector ==

Test the connector by seeing messages come in as we change records in the demo-app's database

1. Start watching the queue that represents database cdc events
----
oc exec -c kafka my-cluster-kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sampledb -n debezium-monitoring --from-beginning --max-messages 1
----
1. open the demo-app
2. Click buy

You should see the following from the consumer window:

or prettified:

image:images/change-event-partial.png[]

== Installing Custom Monitoring ==

The debezium connector we deployed has also the JMX plugin installed within it that allows it to expose metrics to openshift.  We'll take advantage of OpenShift's custom ServiceMonitor (new in OpenShift 4.3) to dump those metrics into the central OpenShift prometheus instance

The metrics were actually enabled in the previous step.  They were defined in the KafkaConnectS2I instance under the `metrics` key

image:images/metrics-exposed.png[]

To setup custom monitoring run:

----
$DEMO_HOME/scripts/02-setup-debezium-connector.sh
----

Which does the following:

1. Activates and enables UserWorkloads by updating `cluster-monitoring-config`
2. waits for user-workload pods to come up successfully in openshift-monitoring
3. creates a ServiceMonitor to scrape metrics out of the debezium connector

== Custom Grafana Dashboard for Debezium ==

image:images/debezium-custom-dashboard-example.png[]